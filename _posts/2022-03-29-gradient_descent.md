---
title: "경사 하강법(Gradient Descent)"
---
Gradient Descent, SGD, Batch

----

안녕하세요.
데이터 사이언티스트를 위한 정보를 공유하고 있습니다.

M1 Macbook Air를 사용하고 있으며, 블로그의 모든 글은 Mac을 기준으로 작성된 점 참고해주세요.

----

# 경사 하강법(Gradient Descent)

이전 포스팅의 회귀의 경우에

데이터를 반복적으로 학습하면서, 즉 w(weight: 가중치)와 b(bias: 편향)를 반복적으로 업데이트 하면서

비용 함수가 최소가 되도록 하는 파라미터(w, b)를 구하는 것이 목적이라고 하였습니다.

이때 최적의 파라미터를 찾아내는 과정의 알고리즘을 **옵티마이저(Optimizer)**라고 부르고,

그중 기본적이며 대표적인 옵티마이저 알고리즘이 바로 **경사 하강법**입니다.

비용 함수의 최솟값을 찾기 위해 기울기를 활용하는 방식이며

인공신경망 기반의 딥러닝을 비롯한 머신러닝에서는 아주 중요한 개념입니다.

----

경사 하강법의 이해를 돕기 위해 비용 함수를 간단히 w에 대한 2차 함수 곡선의 형태로 나타내면 다음 그림과 같을 것입니다.

<img src="../images/2022-03-29-gradient_descent/gradient_descent.png" alt="gradient_descent" style="zoom:50%;" />

이때 처음 위치에서 기울어진 방향으로 학습을 반복하여 오류를 줄여나가며 최소 지점을 찾아내는 것이 경사 하강법입니다.

물론 기울기가 0이 되는 지점이 반드시 최솟값을 나타내진 않습니다.

그림과 달리 비용 함수는 대체로 훨씬 복잡한 형태를 띄고 있으며

따라서 극솟값(기울기가 0인 지점)은 실제로 여러 개 존재할 것입니다.

다만 이번 포스팅에서는 경사 하강법이 어떤 방식으로 진행되는지만 기억해주시길 바랍니다.

----

이번에는 수식을 통해 살펴보겠습니다.

머신러닝과 딥러닝에서 분류, 회귀 등 다양한 문제가 있고 각각의 문제에 적합한 비용 함수가 따로 있습니다.

통칭해서 비용 함수를 cost(w)라고 하겠습니다.

이때 w가 업데이트 되는 방식은 다음과 같습니다.

<img src="../images/2022-03-29-gradient_descent/weightupdate.png" alt="weightupdate" style="zoom:50%;" />

비용 함수의 접선의 기울기에 η(learning rate: 학습률)를 곱한 값을

기존의 w에서 빼준 값이 새로운 w가 됩니다.

접선의 기울기는 w의 위치에 따라 음수일 수도 양수일 수도 있습니다.

하지만 어느 경우든 위의 방식대로 계산된다면 w는 항상 기울기가 0인 방향으로 업데이트 됩니다.

이때 편미분 된 값이 너무 클 수 있기 때문에 이를 보정하기 위한 보정 계수가 바로 학습률 η 입니다.

w가 새로 학습(업데이트)될 때마다 얼마나 크게 변경될 것인지 결정하는,

즉 그림으로 보면 w가 얼마나 큰 폭으로 움직일지 결정하는 값이며,

일반적으로 0과 1사이의 값으로 설정합니다.

학습률이 너무 큰 경우에는 다음 그림과 같이 발산하여 오히려 최소점에서 멀어지는 현상(overshooting)이 발생합니다.

<img src="../images/2022-03-29-gradient_descent/biglearningrate.png" alt="biglearningrate" style="zoom:50%;" />

반대로 너무 작은 경우에는 그만큼 학습 시간이 지나치게 오래 걸리게 됩니다.

따라서 학습률은 충분히 작게, 하지만 지나치게 작지는 않게 적당한 값으로 설정해주어야 합니다.

또한 실제로 비용 함수는 복잡하여 w0, w1, w2, ... 등 변수가 더 다양할 수 있으며

이때의 기울기 값은 각 w에 대하여 편미분하여 계산합니다.

실제 경사하강법에서는 w과 b가 모두 업데이트 된다는 것을 기억해주시길 바랍니다.

----

# SGD(Stochastic Gradient Descent: 확률적 경사 하강법)

일반적인 경사 하강법은 학습을 진행할 때

모든 데이터 샘플마다 각각 파라미터를 업데이트하므로 시간이 매우 오래 걸린다는 단점이 있습니다.

이를 해결하기 위해 전체 데이터를 여러 개의 그룹으로 나누어서 그룹 별로 파라미터를 일괄 업데이트 하는데

이러한 방식이 바로 SGD 알고리즘입니다.

일반적으로 데이터가 큰 실전에서는 확률적 경사 하강법을 사용합니다.

----

# Batch algorithm vs Online algorithm

SGD를 설명할 때 여러 개의 그룹으로 나눈다고 설명하였는데

여기서의 그룹을 딥러닝에서는 배치(batch)라고 표현합니다.

Batch algorithm과 Online algorithm은 대조되는 개념인데요.

Online algorithm는 각각의 데이터 샘플 별로 오류(비용 함수)를 계산하고, 파라미터를 업데이트하는 방식이고,

Batch algorithm은 전체 데이터를 여러 배치로 나누어 각 배치에 포함된 데이터의 오류값을 모두 합산하고, 배치 별로 한 번에 파라미터를 업데이트하는 방식입니다.

예를 들어 데이터의 샘플 수가 10,000개면,

Online algorithm은 각 데이터 샘플 별로 학습이 진행되어 w 업데이트가 10,000번 진행되고,

배치의 개수(mini batch size)를 100으로 지정한 Batch algorithm은 각 배치 별로 한 번씩, 즉 w 업데이트가 100번만 진행되는 것입니다.

또 다른 재밌는 예시로 사람의 뇌는 어떤 알고리즘으로 지식과 정보를 학습하는지 이야기 해볼까요?

저는 두 가지 모든 방식이 적용된다고 생각합니다.

우리 인간은 당장 정보를 보고 들으면서 그때그때 뇌에 업데이트 하지만(Online algorithm),

인간의 뇌는 수면할 때 하루 동안 학습한 내용을 일부 모아서 더 의미있는 정보를 만든다고도 합니다(Batch algorithm).

----

읽어주셔서 감사합니다.

정보 공유의 목적으로 만들어진 블로그입니다.

미흡한 점은 언제든 댓글로 지적해주시면 감사하겠습니다.

----
